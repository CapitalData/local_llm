{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4180e5f-4bdc-488a-a984-69eb9b1d44f8",
   "metadata": {},
   "source": [
    "from network chuck \n",
    "https://www.youtube.com/watch?v=Wjrdr0NU4Sk&t=1236s\n",
    "\n",
    "sudo apt update\n",
    "sudo apt upgrade -y\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "\n",
    "#put this in a browser. This is your endpoint.\n",
    "localhost:11434\n",
    "\n",
    "look for \"ollama is running\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1f5d1f3-60af-4916-b0aa-295a1aa55d6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (657957614.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    ollama pull llama3.1\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## pull and run your model - do this in terminal\n",
    "\n",
    "ollama pull llama3.1\n",
    "run llama3.1\n",
    "\n",
    "#then ask a question in terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94cfd043-9789-4d9a-b1a3-32ed747b76ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G⠋ \u001b[?25h\u001b[?25l\u001b[?25l\u001b[2K\u001b[1G\u001b[?25h\u001b[2K\u001b[1G\u001b[?25h\u001b[?2004h>>> \u001b[38;5;245mSend a message (/? for help)\u001b[28D\u001b[0m\u001b[K\n",
      "Use Ctrl + d or /bye to exit.\n",
      ">>> \u001b[38;5;245mSend a message (/? for help)\u001b[28D\u001b[0m\u001b[K\n",
      ">>> \u001b[38;5;245mSend a message (/? for help)\u001b[28D\u001b[0m"
     ]
    }
   ],
   "source": [
    "wsl # to open a new window to profile your GPU\n",
    "watch -n 0.5 nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8484cf41-ee92-45c9-9d6d-089cb5402a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from there we install and run open-webui\n",
    "need docker\n",
    "\n",
    "\n",
    "#Install Docker\n",
    "# Add Docker's official GPG key:\n",
    "sudo apt-get update\n",
    "sudo apt-get install ca-certificates curl\n",
    "sudo install -m 0755 -d /etc/apt/keyrings\n",
    "sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc\n",
    "sudo chmod a+r /etc/apt/keyrings/docker.asc\n",
    "\n",
    "# Add the repository to Apt sources:\n",
    "echo \\\n",
    "\"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\\n",
    "$(. /etc/os-release && echo \"$VERSION_CODENAME\") stable\" | \\\n",
    "sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\n",
    "sudo apt-get update\n",
    "\n",
    "#Install Dockersudo \n",
    "\n",
    "sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n",
    "     \n",
    "#Run Open WebUi Docker Container\n",
    "docker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e6c55b",
   "metadata": {},
   "source": [
    "### Reactors panel: Ollama endpoint\n",
    "- Add a new reactor card titled **Ollama Endpoint** that mirrors the existing reactor shell (header, status badge, Start/Stop buttons, log console, and quick actions).\n",
    "- Wire the **Start** button to run the install/start script shown below; it checks for `ollama` and installs it only when needed before calling `ollama serve` bound to `0.0.0.0:11434`.\n",
    "- Keep the **Stop** button tied to the same process manager used by other reactors (e.g., `systemctl`, `supervisord`, or your `reactor_manager.py`) so status is always in sync.\n",
    "- Surface health in the status badge by pinging `http://127.0.0.1:11434/api/tags` every few seconds; green when the request returns 200, amber while booting, red on failure.\n",
    "- Reuse the log console region to stream the stdout/stderr from the helper process so operators can watch the install or serve output without leaving the panel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91af632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Helper routine for the Ollama reactor panel\"\"\"\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "import requests\n",
    "OLLAMA_INSTALL_SCRIPT = \"https://ollama.com/install.sh\"\n",
    "OLLAMA_HOST = os.getenv(\"OLLAMA_HOST\", \"0.0.0.0\")\n",
    "OLLAMA_PORT = int(os.getenv(\"OLLAMA_PORT\", 11434))\n",
    "OLLAMA_ENDPOINT = f\"http://127.0.0.1:{OLLAMA_PORT}\"\n",
    "def ensure_ollama() -> Tuple[bool, str]:\n",
    "    \"\"\"Download and install Ollama if it is missing.\"\"\"\n",
    "    if shutil.which(\"ollama\"):\n",
    "        return True, \"ollama already installed\"\n",
    "    install_cmd = f\"curl -fsSL {OLLAMA_INSTALL_SCRIPT} | sh\"\n",
    "    completed = subprocess.run(install_cmd, shell=True, text=True)\n",
    "    if completed.returncode != 0:\n",
    "        return False, \"ollama install failed\"\n",
    "    return True, \"ollama installed\"\n",
    "def start_ollama_daemon():\n",
    "    \"\"\"Start or reuse the ollama serve process bound to the configured host/port.\"\"\"\n",
    "    env = os.environ.copy()\n",
    "    env[\"OLLAMA_HOST\"] = OLLAMA_HOST\n",
    "    env[\"OLLAMA_PORT\"] = str(OLLAMA_PORT)\n",
    "    return subprocess.Popen([\"ollama\", \"serve\"], env=env)\n",
    "def wait_for_health(timeout: int = 30) -> bool:\n",
    "    \"\"\"Poll the tags endpoint until the service responds.\"\"\"\n",
    "    deadline = time.time() + timeout\n",
    "    while time.time() < deadline:\n",
    "        try:\n",
    "            response = requests.get(f\"{OLLAMA_ENDPOINT}/api/tags\", timeout=2)\n",
    "            if response.ok:\n",
    "                return True\n",
    "        except requests.RequestException:\n",
    "            time.sleep(1)\n",
    "    return False\n",
    "if __name__ == \"__main__\":\n",
    "    ok, msg = ensure_ollama()\n",
    "    print(msg)\n",
    "    if ok:\n",
    "        process = start_ollama_daemon()\n",
    "        print(f\"ollama serve started with pid {process.pid}\")\n",
    "        if wait_for_health():\n",
    "            print(f\"✅ Ollama ready at {OLLAMA_ENDPOINT}\")\n",
    "        else:\n",
    "            print(\"⚠️ Ollama did not pass health checks in time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6375a600",
   "metadata": {},
   "source": [
    "### Reactors panel: Interactive Ollama terminal window\n",
    "- Create a sibling reactor window labeled **Ollama Terminal** that shares the same header, status display, and `Open Console`/`Close Console` controls already used in other terminal reactors.\n",
    "- Route its iframe/webview to the WebSocket/terminal service your dashboard already hosts (e.g., `reactor_console.py`). Point the backend process at `python ollama_terminal.py` (script below) so the UI behaves exactly like the other terminal panes.\n",
    "- When the main panel reports that the endpoint is healthy, automatically focus this terminal and seed it with a greeting message explaining the available commands (e.g., `/model`, `/clear`, `/quit`).\n",
    "- Persist the last 50 prompts/responses so that reopening the terminal restores context for the on-call engineer. You can reuse the existing `reactor_logs` store that other panels use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7283b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Simple interactive console for the Ollama terminal reactor\"\"\"\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "OLLAMA_PORT = int(os.getenv(\"OLLAMA_PORT\", 11434))\n",
    "OLLAMA_CHAT_URL = f\"http://127.0.0.1:{OLLAMA_PORT}/api/chat\"\n",
    "DEFAULT_MODEL = os.getenv(\"OLLAMA_MODEL\", \"llama3.1\")\n",
    "def stream_chat(prompt: str, model: str) -> str:\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"stream\": False\n",
    "    }\n",
    "    response = requests.post(OLLAMA_CHAT_URL, json=payload, timeout=120)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    return data.get(\"message\", {}).get(\"content\", \"<no response>\")\n",
    "def run_terminal():\n",
    "    print(\"Interactive Ollama terminal. Type /quit to exit, /model <name> to switch models.\")\n",
    "    model = DEFAULT_MODEL\n",
    "    while True:\n",
    "        user_input = input(\"You> \").strip()\n",
    "        if not user_input:\n",
    "            continue\n",
    "        if user_input.lower() in {\"/quit\", \"exit\", \"quit\"}:\n",
    "            print(\"Ending session.\")\n",
    "            break\n",
    "        if user_input.startswith(\"/model\"):\n",
    "            parts = user_input.split(maxsplit=1)\n",
    "            if len(parts) == 2:\n",
    "                model = parts[1].strip() or model\n",
    "                print(f\"Model switched to {model}\")\n",
    "            else:\n",
    "                print(\"Usage: /model <model_name>\")\n",
    "            continue\n",
    "        if user_input == \"/clear\":\n",
    "            os.system(\"clear\")\n",
    "            continue\n",
    "        try:\n",
    "            answer = stream_chat(user_input, model)\n",
    "            print(f\"Ollama> {answer}\")\n",
    "        except requests.RequestException as exc:\n",
    "            print(f\"⚠️ Request failed: {exc}\")\n",
    "if __name__ == \"__main__\":\n",
    "    run_terminal()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "notre_dame",
   "language": "python",
   "name": "notre_dame"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
